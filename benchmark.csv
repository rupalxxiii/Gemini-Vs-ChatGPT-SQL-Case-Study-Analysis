BenchmarkID,ModelID,CapabilityID,BenchmarkName,ScoreGemini,ScoreGPT4,Description
1,1,1,MMLU,90,86.4,"Representation of questions in 57 subjects"
2,2,1,MMLU,86.4,NULL,"Representation of questions in 57 subjects"
3,1,2,"Big-Bench Hard",83.6,83.1,"Diverse set of challenging tasks requiring multi-step reasoning"
4,2,2,"Big-Bench Hard",83.1,NULL,"Diverse set of challenging tasks requiring multi-step reasoning"
5,1,2,DROP,82.4,80.9,"Reading comprehension (Fl Score)"
6,2,2,DROP,80.9,NULL,"Reading comprehension (Fl Score)"
7,1,2,HellaSwag,87.8,95.3,"Commonsense reasoning for everyday tasks"
8,2,2,HellaSwag,95.3,NULL,"Commonsense reasoning for everyday tasks"
9,1,3,GSM8K,94.4,92,"Basic arithmetic manipulations, incl. Grade School math problems"
10,2,3,GSM8K,92,NULL,"Basic arithmetic manipulations, incl. Grade School math problems"
11,1,3,MATH,53.2,52.9,"Challenging math problems, incl. algebra, geometry, pre-calculus, and others"
12,2,3,MATH,52.9,NULL,"Challenging math problems, incl. algebra, geometry, pre-calculus, and others"
13,1,4,HumanEval,74.4,67,"Python code generation"
14,2,4,HumanEval,67,NULL,"Python code generation"
15,1,4,Natura12Code,74.9,73.9,"Python code generation. New held out dataset HumanEval-like, not leaked on the web"
16,2,4,Natura12Code,73.9,NULL,"Python code generation"
17,1,5,MIMMU,59.4,56.8,"Multi-discipline college-level reasoning problems"
18,2,5,VQAv2,77.8,77.2,"Natural image understanding"
19,1,5,TextVQA,82.3,78,"OCR on natural images"
20,2,5,DocVQA,90.9,88.4,"Document understanding"
21,1,5,"Infographic VQA",80.3,75.1,"Infographic understanding"
22,2,5,MathVista,53,49.9,"Mathematical reasoning in visual contexts"
23,1,6,VATEX,62.7,56,"English video captioning (CIDEr)"
24,2,6,"Perception Test MCQA",54.7,46.3,"Video question answering"
25,1,7,"CoV0ST 2",40.1,29.1,"Automatic speech translation (BLEU score)"
26,2,7,FLEURS,7.6,17.6,"Automatic speech recognition (word error rate)"
